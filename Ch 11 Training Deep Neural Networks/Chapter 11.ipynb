{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "226547da",
   "metadata": {},
   "source": [
    "# Chapter 11: Training Deep Neural Networks\n",
    "We looked at shallow and simple, deep neural networks. What if we want to model a complex problem like detecting hundreds of objects in high-res images? We will need much deeper neural networks.\n",
    "\n",
    "Some problems:\n",
    "1. Vanishing and exploding gradient problems that makes lower levels hard to train\n",
    "2. not enough training data or too costly to label\n",
    " - solution: transfer learning and unsupervised pretraining\n",
    "3. training extremely slow\n",
    " - solution: different optimizers\n",
    "4. risk of severly overfitting the training set because of millions of parameters\n",
    " - solution: regularization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155a2570",
   "metadata": {},
   "source": [
    "## Vanishing and Exploding Gradient Problems\n",
    "Recall, the backpropogation algorithm goes from the output layer to the input layer, propogating the error gradient along the way. Once the algorithm has computed the gradient of the cost function with respect to each parameter (connection) in the network, it uses these gradients to update each parameter with a gradient descent step. \n",
    "\n",
    "But the gradients often get smaller and smaller as the algorithm progresses to the lower layers (near the input); therefore the lower layer connection weights are virtually unchanged and the problem never converges to a good solution; called the **vanishing gradient problem**.\n",
    "\n",
    "**Exploding Gradient** is the opposite problem, where the gradient diverges because the connection weights get extremely large. This mostly happens with RNNs.\n",
    "\n",
    "More generally, neural networks may suffer from unstable gradients where the layers learn at much different speeds.\n",
    "\n",
    "Glorot and Bengio write in a 2010 paper that a culprit to gradient problems is the sigmoid logistic function and random initialization combination: that as the neural network \"feeds forward\" the variances increases, then becomes saturated. The saturation is that the derivative is too close to 0 or 1 and the gradient either vanishes or diverges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4d89b8",
   "metadata": {},
   "source": [
    "### Glorot and He Initialization\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
