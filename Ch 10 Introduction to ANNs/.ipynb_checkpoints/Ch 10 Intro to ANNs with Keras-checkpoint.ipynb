{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec952777",
   "metadata": {},
   "source": [
    "# Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
    "ANNs are at the core of deep learning.\n",
    "\n",
    "This chapter covers:\n",
    " - the very $1^{st}$ ANN architectures\n",
    " - Multi-layer perceptrons\n",
    " - Implementation with Keras\n",
    " \n",
    "Keras: API for building, training, evaluating, and running different types of ANNs. Chapter 12 covers modifying/creating different models in Keras, but Keras lower-level API is good to know for extra flexibility.\n",
    "\n",
    "## From Biological to Artificial Neurons\n",
    "- First ANN architecture introduced by McCulloch and Pitts in 1943\n",
    " - presented a simple, computational model of how biological neurons might work\n",
    " - used propositional logic\n",
    "- ANN research went dormant.\n",
    "- ANNs re-emerged because:\n",
    " - lots more data available\n",
    " - ANNs frequently outperform other ML algorithms on large and complex problems.\n",
    " - huge increase in computing power\n",
    " - training algorithms have improved\n",
    " - theoretical limitations are frequently benign\n",
    " \n",
    " \n",
    "### Biological Neurons\n",
    "Biological neurons are organized as: networks of billions, each neuron is connected to others, they are seemingly **organized in consecutive layers**, and a neuron(s) firing signals other neuron[s] to fire.\n",
    "\n",
    "Note: Connectionism is the study of Neural Networks.\n",
    "\n",
    "\n",
    "### Logical Computations with Neurons\n",
    "The model proposed by McCulloch and Pitts is called the **artificial neuron**.\n",
    "\n",
    "The **artificial neuron**:\n",
    " - one or more binary inputs\n",
    " - one binary output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b5a2ff",
   "metadata": {},
   "source": [
    "## The Perceptron\n",
    "One of the simplest ANN architectures.\n",
    "\n",
    "Based on an artificial neuron called the *threshold logic unit* (or *linear threshold unit*) that is different to the one using propositional logic.\n",
    "\n",
    "**Threshold Logic Unit** (TLU) (better type of artificial neuron)\n",
    "- inputs are now numbers (instead of binary)\n",
    "- input connections associated with a weight\n",
    "- computes weighted sum of inputs; z (linear equation)\n",
    " - $z = w_1x_1 + w_2x_2 + ... + w_nx_n$\n",
    "- applies a step function to z\n",
    " - $step(z)$; (nonlinear) transformation\n",
    "\n",
    "Common step functions used in the perceptron:\n",
    " - $heaviside(z)$ = {0 if $z < 0$, 1 if $z \\geq 0$}\n",
    " - $sgn(z)$ = {-1 if z < 0, 0 if z = 0, 1 if z > 0}\n",
    " \n",
    "Training a TLU in this case means finding the values for the weights that fit the training data.\n",
    "\n",
    "Note: **A perceptron is a layer of TLUs. The TLU is a single artificial neuron.**\n",
    "\n",
    "**Def'n** Dense Layer: When all the neurons in a layer are connected to all the neurons in the previous layer\n",
    " - aka \"fully connected\" layer\n",
    " \n",
    "**Def'n** Input neurons: form the input layer and are just the input data.\n",
    "\n",
    "**Def'n** Bias Neuron: An extra bias feature added to the Perceptron.\n",
    "\n",
    "The Greek letter $\\phi$ is the notation for the activation function.\n",
    "\n",
    "Note: The activation function for a Perceptron is the step function\n",
    "\n",
    "How is a perceptron trained? The connection weight between two neurons is increased when they have the same output. Connections that help reduce the error are reinforced. The Perceptron is fed one training instance at a time, and for each instance it makes its predictions. For every output neuron that produced a wrong prediction, it reinforces the connection weights from inputs that would have contributed to the correct prediction.\n",
    "\n",
    "*Perceptron learning rule* (weight update):\n",
    "$w_{i,j}^{nextStep} = w_{i,j} + \\eta(y_i - \\hat{y_i})x_i$\n",
    "- $w_{i,j}$ is the connection weight for the i-th input neuron and j-th output neuron.\n",
    "- $\\eta$ is the learning rate.\n",
    "- $y_i$ is the target output of the j-th output neuron for the current training instance.\n",
    "- $\\hat{y_i}$ is the output of j-th output neuron for the current training instance.\n",
    "- $x_i$ is the value of the current training instance.\n",
    "\n",
    "Perceptrons are incapable of learning complex patterns because the output is linear unless the data is linearly separable. This is called the *Perceptron Convergence Theorem*.\n",
    "\n",
    "SK-learns `Perceptron` class provides a single TLU network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b40a4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Perceptron()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Perceptron</label><div class=\"sk-toggleable__content\"><pre>Perceptron()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # petal length, petal width\n",
    "y = (iris.target == 0).astype(int) # Iris Setosa?\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "277bb6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a8f7ab",
   "metadata": {},
   "source": [
    "Notice that Perceptron's learning algorithm strongly resembles that of Stochastic Gradient Descent (SGD); they are equivalent when tweaking SGD's hyperparameters to loss=\"perceptron\", learning_rate=\"constant\", eta0=1 (the learning rate), and penalty=None (no regularization).\n",
    "\n",
    "Perceptrons do not output class probabilities, unlike Logistic Regression (LRC).\n",
    "\n",
    "People critiqued Perceptrons because of some limitations, but these were largely eliminated by stacking Perceptrons: a concept called **Multi-layer Perceptron (MLP)** (try exercise on p285 to verify the XOR problem is solved with MLPs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42edab84",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron and Backpropogation\n",
    "MLP composition:\n",
    " - one input layer (passthrough layer)\n",
    " - one or more layers of TLUs: hidden layer\n",
    " - one final layer of TLUs: output layer\n",
    " - every layer except output contains bias neuron\n",
    " - every layer is fully connected (for now)\n",
    " \n",
    "Layers close to the input layer are called *lower layers*, layers closer to the output layer are *upper layers*. Signals only flow forward, so this is a **feedforward neural network**. An ANN with two or more hidden layers is called a *Deep Neural Network*. \n",
    "\n",
    "MLPs are trained using **backpropogation** which is simply Gradient Descent that automatically computes gradients. It passes forward, then backward, and is able to compute the gradient of the network’s error with regards to every single model parameter; it finds out how each connection weight and bias term should be changed to reduce the error. Once it has all the gradients, it just performs gradient descent.\n",
    "\n",
    "Note: Automatically computing gradients is called *autodiff*.\n",
    "\n",
    "In detail, backpropogation:\n",
    " - Handles the training set in mini-batches and goes through the full training set multiple times; each pass is called an epoch.\n",
    " - Each mini-batch is sent to the input layer, but recall the input layer just passes to the first hidden layer. The output is computed at layer i is passed as input to layer j, then the layer j output is passed as input into layer k, and so on until it reaches the true \"output\" layer. All intermediate results (at each layer) are stored for backpropogation. This is the forward pass.\n",
    " - Next, the algorithm measure's the network's output error. It uses a loss function that compares the desired (true) outputs to the actual outputs (predictions) of the network.\n",
    " - It then computes how much each output contributed to the error. This is done by applying the *chain rule* from calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a72bce9",
   "metadata": {},
   "source": [
    "For the above algorithm to work properly, the authors used the logistic function instead of the step function. The step function only has flat segments and the gradient cannot be computed. The logistic function has a well-defined, nonzero derivative everywhere. Backpropogation works with other activation functions as well. Two other popular activation function are:\n",
    "\n",
    "- Tanh\n",
    " - The output values range from -1 to 1, so the mean is centered at 0 and convergence is faster.\n",
    " \n",
    "- RELU\n",
    " - Fast to compute and does not have a maximum, which reduces Gradient Descent issues.\n",
    " \n",
    "Activation functions are used to create nonlinearity between layers, \n",
    "\n",
    "Chaining linear transformations together results in a linear transformation, therefore **activation functions** *are used to create nonlinearity between layers*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba1ca5e",
   "metadata": {},
   "source": [
    "## Regression MLPs\n",
    "If you want to predict a single value, say, the price of a house given many features, then you just need a single output neuron. \n",
    "\n",
    "For multivariate regression, predicting multiple outputs at once, you need one output neuron per dimension. EX: Locating the center of an object in an image requires predicting 2D coordinates, so you need two output neurons. Placing a bounding box around that object will require two more numbers: the width and height of the image; you will have 4 output neurons.\n",
    "\n",
    "When building a Regression MLP, you **do not want to use an activation function** so the *output can have any range of values*. Or to guarantee all outputs are positive, use RELU (or softplus). To guarantee that outputs fall into a range of values, use the logistic or tanh activations and scale the labels to [0,1] for logistic and [-1,1] for tanh.\n",
    "\n",
    "The typical loss function:\n",
    " - MSE (mean squared error)\n",
    " - MAE (mean absolute error) (if outliers)\n",
    " - Huber loss (combination of MSE and MAE)\n",
    "  - Huber is less sensetive to outliers that MSE, but converges faster and is more precise than MAE.\n",
    " \n",
    "\n",
    "Table 10-1: Typical Regression MLP architecture:\n",
    "\n",
    "|Hyperparameter| Typical Value|\n",
    "|--------------|--------------|\n",
    "| # input neurons| One per input feature|\n",
    "| # hidden layers| Depends on problem. Typically 1-5|\n",
    "| # neurons per hidden layer| Depends on problem. Typically 1-100|\n",
    "| # of output neurons| 1 per prediction dimension|\n",
    "| Hidden activation| RELU (or SELU)|\n",
    "| Output activation| **None** or RELU/softplus (positive outputs) or Logistic/tanh (if bounded)|\n",
    "| Loss function| MSE or MAE/Huber (if outliers)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c655fd8",
   "metadata": {},
   "source": [
    "## Classification MLPs\n",
    "For **binary classification**, use a single output neuron with the logistic activation: the output will be between 0 and 1, and is the P(positive class). Note: P(negative class) = 1-P(positive class)\n",
    "\n",
    "MLPs also handle **multilabel binary classification**: An example being an email classification system that predicts whether an incoming email is spam or ham AND predicts whether it is urgent or non-urgent. In this case, use two output neurons with the logistic activation. The first neuron outputs the probability that the email is spam, and the second neuron outputs the probability it is urgent.\n",
    "Note that probabilities do not have to add up to one as they are different classes: if our two neurons output 20% chance the email being spam and 20% chance of the email being urgent, then we say the email is non-spam and non-urgent.\n",
    "\n",
    "We could even have a case where an email is urgent spam! But this would probably be an error.\n",
    "\n",
    "If each instance belongs to a single class out of 3 or more possible classes, then one output neuron per class is needed and we will need to use the softmax activation function for the output layer as it ensures the probabilities are between 0 and 1 and add up to 1 for each prediction. This is called **multiclass classification**. Regarding the loss function, since we use probability distributions, we usually use cross-entropy (aka log loss). \n",
    "\n",
    "|Hyperparameter| Binary Classification| Multilabel Binary clf| Multiclass classification|\n",
    "|--------------|----------------------|----------------------|--------------------------|\n",
    "| Input and hidden layers| Same as regression| Same as regression| Same as regression|\n",
    "| # output neurons| 1| 1 per label| 1 per class|\n",
    "| Output layer activation| Logistic| Logistic| Softmax|\n",
    "| Loss function| Cross-entropy| Cross-entropy| Cross-entropy|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3699dbec",
   "metadata": {},
   "source": [
    "## Implementing MLPs with Keras\n",
    "Keras is a high-level Deep Learning API that allows ease in building, training, evaluating, and executing all sorts of Neural Networks!\n",
    "\n",
    "Keras requires computation backends: 3 popular ones (there are a bunch more) are Tensorflow, Theano, and Microsoft Cognitive Toolkit (CNTK). We will use Tensorflow as Tensorflow's Data API makes it easy to load and preprocess data.\n",
    "\n",
    "Note: Code examples with Keras.io will need different imports when using tensorflow (or maybe other backends)\n",
    "\n",
    "### Building an Image Classifier Using the Sequential API\n",
    "We will build an image classifier for Fashion MNIST, which is similar to MNIST in that the images are 28x28, but the images are of clothes so the problem is more complex.\n",
    "\n",
    "#### Using Keras to Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cb3793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac7b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "640ad345",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e84819c",
   "metadata": {},
   "source": [
    "When loading MNIST or fashion MNIST from Keras rather than SKlearn, the images are already represented as 28x28, instead of a 784 length vector, and the values are integers instead of floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41d9d1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "print(X_train_full.shape)\n",
    "print(X_train_full.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283b97ac",
   "metadata": {},
   "source": [
    "The dataset is, by default, split into training and testing sets, but we should create a validation set. We do this by further splitting the training set. We will also scale the images to a [0,1] range by dividing them by 255 (the max pixel intensity); the neural network needs scaled features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8998fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 5k observations go into the validation set\n",
    "# The rest go into the training set\n",
    "# Note we scale the training features\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b8dc3",
   "metadata": {},
   "source": [
    "We need a list of the labels since numeric outputs will not be super descriptive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da303f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    " \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2b5ed0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "\n",
      "Coat\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])\n",
    "print()\n",
    "print(class_names[y_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e82f485",
   "metadata": {},
   "source": [
    "#### Creating the Model Using the Sequential API\n",
    "We will create a multiclass classification MLP with the Sequential API using two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eba19b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()                        # Initialize the model\n",
    "model.add(keras.layers.Flatten(input_shape = [28,28]))   # Input Layer\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))    # Hidden layer with 300 neurons\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))    # Hidden layer with 100 neurons\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))    # Output layer with 1 node for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac6141c",
   "metadata": {},
   "source": [
    "Note the layer with `(keras.layers.Flatten(input_shape = [28,28])` is the input layer flattened to be 784 length, one input neuron for each feature; 784 features per image.\n",
    "\n",
    "Line-by-line:\n",
    "\n",
    "1. `model = keras.models.Sequential()` is the simplest kind of Keras model. It is for Neural Networks with a single stack of layers\n",
    "2. `model.add(keras.layers.Flatten(input_shape = [28,28]))` is the input layer (the first layer). Flatten flattens the image to a 1D vector.\n",
    "3. `model.add(keras.layers.Dense(300, activation=\"relu\"))` is a Dense, hidden layer with the RELU activation.\n",
    "4. `model.add(keras.layers.Dense(100, activation=\"relu\"))` is a Dense, hidden layer with the RELU activation.\n",
    "5. `model.add(keras.layers.Dense(10, activation=\"softmax\"))` is the Dense, output layer with one neuron per class, and softmax because classes are exclusive.\n",
    "\n",
    "Note: `activation = \"relu\"` is equivalent to `activation = keras.activations.relu`\n",
    "\n",
    "Alternatively, we can use the sequential API as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fed551d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    " keras.layers.Flatten(input_shape=[28, 28]),\n",
    " keras.layers.Dense(300, activation=\"relu\"),\n",
    " keras.layers.Dense(100, activation=\"relu\"),\n",
    " keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3491ae26",
   "metadata": {},
   "source": [
    "The model's `.summary()` method displays info about all the model's layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d9b4c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c639f8d",
   "metadata": {},
   "source": [
    "- Layer is the layer name (automatically generated)\n",
    "- Output Shape is the number of neurons\n",
    "- parameters is the number of connections between that layer and the previous.\n",
    " - The first layer is 0 because there is not a previous layer to connect.\n",
    " \n",
    "This example only has trainable parameters. See ch. 11 for nontrainable parameters.\n",
    "\n",
    "Dense layers will have lots of parameters (connections). Between the input layer and the first hidden layer, there are 235500 connections (parameters), which is (784 x 300) for the neurons between the two layers, and 300 bias terms.\n",
    "\n",
    "This model is quite flexible, but prone to overfitting without enough training data.\n",
    "\n",
    "To *get a list of layers in a model*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69f59c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Flatten at 0x27b374a32e0>,\n",
       " <keras.layers.core.Dense at 0x27b374a3f10>,\n",
       " <keras.layers.core.Dense at 0x27b374a3b80>,\n",
       " <keras.layers.core.Dense at 0x27b374a31f0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers #get a list of layers in a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab811ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_3'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.layers[1].name # or by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9f40c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_3'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.get_layer('dense_3').name # or by name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce16197",
   "metadata": {},
   "source": [
    "All parameters of a layer can be accessed by the `.get_weights()` and `.set_weights()` methods.\n",
    "\n",
    "For a Dense layer, this includes the connections and the bias terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "865ed77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55ec0c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a0bf217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11094d9b",
   "metadata": {},
   "source": [
    "Note that when viewing the `weights` vector, all the values are initialized randomly.\n",
    "\n",
    "The biases were all initialized to zeros, which is fine.\n",
    "\n",
    "To use different initialization methods, use:\n",
    "\n",
    "- `kernel_initializer`\n",
    " - connection weights; Kernel is another name for the matrix of connection weights\n",
    "- `bias_initializer`\n",
    " - for biases\n",
    " \n",
    "The shape of the weight matrix depends on the number of inputs (number of features). It is recommended to specify the input shape to be able to view the summary or save a model; otherwise it will wait until it knows the input shape to actually build the model. When the model is built, then the weights become initialized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe42cfc",
   "metadata": {},
   "source": [
    "#### Compiling the Model\n",
    "After creating the model, we must call `.compile()` to specify the loss function, optimizer, and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "328bd23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    " optimizer=\"sgd\",\n",
    " metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ae476",
   "metadata": {},
   "source": [
    "We use `sparse_categorical_crossentropy` because we have sparse labels; one target class index from 0 to 9, and classes are exclusive.\n",
    "\n",
    "If instead we had one target probability per class for each instance (such as one-hot vectors, e.g. [0.,0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent class 3), then we would need to use the \"categorical_crossentropy\" loss instead. ?????????????? P299.\n",
    "\n",
    "Doing binary classification with one or more binary labels, we use the sigmoid (logistic) activation with the loss as `binary_crossentropy`. \n",
    "\n",
    "You can convert sparce labels like indices to one-hot vector labels with keras.utils.to_categorical(), and reverse the process with np.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27063f1d",
   "metadata": {},
   "source": [
    "#### Training and Evaluating the Model\n",
    "We now fit (train) the data.\n",
    "\n",
    "Other than the training data, the model needs a number of epochs, and optionally the validation set.\n",
    "\n",
    "If the model performs much better on the training data than on the validation, then the model is overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2269142d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.7238 - accuracy: 0.7608 - val_loss: 0.5022 - val_accuracy: 0.8284\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.4872 - accuracy: 0.8300 - val_loss: 0.4484 - val_accuracy: 0.8518\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4416 - accuracy: 0.8458 - val_loss: 0.4374 - val_accuracy: 0.8464\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4135 - accuracy: 0.8552 - val_loss: 0.3907 - val_accuracy: 0.8672\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3938 - accuracy: 0.8614 - val_loss: 0.3781 - val_accuracy: 0.8732\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3789 - accuracy: 0.8656 - val_loss: 0.4043 - val_accuracy: 0.8598\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3649 - accuracy: 0.8719 - val_loss: 0.3587 - val_accuracy: 0.8736\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3538 - accuracy: 0.8745 - val_loss: 0.3512 - val_accuracy: 0.8792\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3437 - accuracy: 0.8780 - val_loss: 0.3559 - val_accuracy: 0.8750\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3345 - accuracy: 0.8797 - val_loss: 0.3399 - val_accuracy: 0.8802\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3275 - accuracy: 0.8831 - val_loss: 0.3396 - val_accuracy: 0.8814\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3180 - accuracy: 0.8862 - val_loss: 0.3352 - val_accuracy: 0.8846\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3119 - accuracy: 0.8881 - val_loss: 0.3204 - val_accuracy: 0.8880\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3046 - accuracy: 0.8908 - val_loss: 0.3665 - val_accuracy: 0.8632\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2980 - accuracy: 0.8944 - val_loss: 0.3216 - val_accuracy: 0.8830\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2918 - accuracy: 0.8955 - val_loss: 0.3214 - val_accuracy: 0.8868\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2856 - accuracy: 0.8976 - val_loss: 0.3314 - val_accuracy: 0.8822\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2815 - accuracy: 0.8982 - val_loss: 0.3131 - val_accuracy: 0.8880\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2757 - accuracy: 0.9007 - val_loss: 0.3084 - val_accuracy: 0.8912\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2711 - accuracy: 0.9023 - val_loss: 0.3113 - val_accuracy: 0.8912\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2654 - accuracy: 0.9047 - val_loss: 0.3149 - val_accuracy: 0.8846\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2610 - accuracy: 0.9069 - val_loss: 0.3058 - val_accuracy: 0.8904\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2567 - accuracy: 0.9069 - val_loss: 0.2995 - val_accuracy: 0.8940\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2526 - accuracy: 0.9083 - val_loss: 0.3092 - val_accuracy: 0.8880\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2479 - accuracy: 0.9109 - val_loss: 0.3028 - val_accuracy: 0.8908\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2439 - accuracy: 0.9114 - val_loss: 0.3044 - val_accuracy: 0.8906\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2397 - accuracy: 0.9140 - val_loss: 0.3047 - val_accuracy: 0.8888\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2358 - accuracy: 0.9157 - val_loss: 0.3163 - val_accuracy: 0.8898\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2321 - accuracy: 0.9166 - val_loss: 0.2941 - val_accuracy: 0.8972\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2287 - accuracy: 0.9187 - val_loss: 0.2969 - val_accuracy: 0.8972\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                     validation_data=(X_valid, y_valid))\n",
    "\n",
    "# Note that running multiple times picks up where the previous run leaves off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863b0ad5",
   "metadata": {},
   "source": [
    "We did it! Also, the validation accuracy is pretty close to the training accuracy, so we are not overfitting the data!\n",
    "\n",
    "Note: instead of `validation_data` with an X and y validation set, we could set `validation_split` to a ratio of the training data and it will automatically split the training set for us.\n",
    "\n",
    "If the training set is very skewed (not the case with MNIST where each of the 10 classes represents 10% of the data), it is useful to set the `class_weight` argument higher with underrepresented classes and lower with overrepresented ones.\n",
    "\n",
    "Computing per-instance weights is set using `sample_weight`. This could be useful if some data is labelled by experts while other labels are done by crowdsourcing: you may want to give more weight to the former.\n",
    "\n",
    "Sample weights (but not class weights) may be added to the validation data by adding a 3rd item to the `validation_data` tuple.\n",
    "\n",
    "The `.fit()` method returns a `history` object containing:\n",
    "\n",
    "- Training params\n",
    " - `history.params`\n",
    "- List of epochs it went through\n",
    " - `history.epoch`\n",
    "- *Dictionary of loss and metrics at each epoch* (most important)\n",
    " - `history.history`\n",
    " - returns metrics for training and validation (only if validation was specified)\n",
    " \n",
    "The `history.history` dictionary converted into a Pandas dataframe may be used to get the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af565a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06a25df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae733a2",
   "metadata": {},
   "source": [
    "If the training and validation accuracy curves are close, there is not much overfitting.\n",
    "\n",
    "Training performance usually ends up beating validation performance when training for long enough.\n",
    "\n",
    "Remark: the loss looks like it is still going down, therefore we should continue training. Calling .fit() again on the model picks up where it left off.\n",
    "\n",
    "Not satisfied with model's performance? Tune the hyperparameters being the number of hidden layers, neurons per layer, activation function per layer, number of epochs, and batch size.\n",
    "\n",
    "Once satisfied with the validation accuracy, evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de839dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 63.6648 - accuracy: 0.8433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[63.664817810058594, 0.8432999849319458]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d117f27d",
   "metadata": {},
   "source": [
    "Remember not to tweak hyperparameters on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0d3a4b",
   "metadata": {},
   "source": [
    "#### Using the Model to Make Predictions\n",
    "We can use the model's `.predict()` method to make predictions on new instances. We don't have new pictures, so we will just use the first 3 instances of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42def731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 28, 28)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72192652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caaec3e",
   "metadata": {},
   "source": [
    "For each instance, the model estimates a probability of each class. In this case, it is just predicting the 10th, 3rd, and 2nd classes as 100%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5014e87",
   "metadata": {},
   "source": [
    "### Regression MLPs using Sequential API\n",
    "We will use SKlearn's california housing data (no missing values, no categorical features) for the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13af9aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Split into training_full and testing sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "\n",
    "# Split training_full into training and validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "# call Scaler transformer\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale all feature data\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Only fit_transform the training data\n",
    "X_valid_scaled = scaler.transform(X_valid) # Only transform the validation data\n",
    "X_test_scaled = scaler.transform(X_test)   # Only transform the testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae4944f",
   "metadata": {},
   "source": [
    "Building the regression MLP is similar to the classification MLP, but recall that we use different loss functions, no activation function, and one output neuron (in this case).\n",
    "\n",
    "In this case, we will use a single hidden layer with fewer neurons to avoid overfitting, because the data is noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0172164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    " keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    " keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffd31be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21b6984e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: inf - val_loss: inf\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: inf - val_loss: 23225094588116240738565011294650368.0000\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1616515411154271880801231577284608.0000 - val_loss: 9910335869709443814647660544.0000\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 689780503100770865503010816.0000 - val_loss: 4228833888204715196416.0000\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 294335427659077517312.0000 - val_loss: 1804475748581376.0000\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 125595430158336.0000 - val_loss: 769983168.0000\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 53592460.0000 - val_loss: 329.5304\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 24.2146 - val_loss: 1.3143\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3244 - val_loss: 1.3119\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3245 - val_loss: 1.3119\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.3244 - val_loss: 1.3117\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3246 - val_loss: 1.3121\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3245 - val_loss: 1.3117\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.3245 - val_loss: 1.3131\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.3245 - val_loss: 1.3121\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.3245 - val_loss: 1.3128\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.3242 - val_loss: 1.3135\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.3246 - val_loss: 1.3126\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3244 - val_loss: 1.3142\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3244 - val_loss: 1.3142\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20,\n",
    " validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b130668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 1.3649\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bf7d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3] # pretend these are new instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b8579c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5954ddb",
   "metadata": {},
   "source": [
    "The Sequential API is good, but sometimes we need more complex models. For that, we need the \"Functional API\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39687159",
   "metadata": {},
   "source": [
    "#### Building Complex Models Using the Functional API\n",
    "An example of a non-sequential neural network is a **deep and wide** neural network.\n",
    "\n",
    "It connects all or part of the inputs directly to the output layer. This architecture makes it possible to learn **deep patterns** (using the deep path, aka sequential path) **or simple rules** through the path directly to the output layer. \n",
    "\n",
    "In contract, a regular MLP forces all data to go through the full stack of layers where simple patterns may be distorted by the sequence of transformations.\n",
    "\n",
    "Let's build a deep and wide neural network on the Cali housing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15318c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.models.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66227be",
   "metadata": {},
   "source": [
    "Line-by-line:\n",
    "\n",
    "1. Create an `input` object because we may have multiple inputs (explained later).\n",
    "2. Next we create a hidden layer. The `input` object called `input_` is passed (connected) to the hidden layer via some syntax that allows a variable behind the function.\n",
    "3. Create another hidden layer with the same syntax, in this case. Pass it the output of the layer before it.\n",
    "4. Next we create a concatenate layer that connect the input layer and the *output* of the second hidden layer. Connecting it to the output of the second hidden layer means it skipped transformations in both hidden layers and will go directly into the Output layer.\n",
    "5. We create an output layer and connect the concatenated layer. Recall this is a univariate regression problem so there is 1 neuron and no activation function.\n",
    "6. Now we create the model and specify inputs and outputs.\n",
    "\n",
    "Now we do the standard procedures: compilation, training (fitting), evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ca3423",
   "metadata": {},
   "source": [
    "But wait!!! We said we can send **multiple inputs**. \"What if we want to send a **subset of features** through the wide path and a different subset (possibly overlapping) through the deep path\". We can use multiple inputs as such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d4758cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5])    # This layer goes wide\n",
    "input_B = keras.layers.Input(shape=[6])    # This layer goes deep\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)  # Connect to B as it goes deep\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)  # Connect to hidden1\n",
    "concat = keras.layers.concatenate([input_A, hidden2])  # Connect A to output of deep layers\n",
    "output = keras.layers.Dense(1)(concat) # Connect to output\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output]) # Note two input layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e72ed28",
   "metadata": {},
   "source": [
    "We compile the model as usual, but when fitting, evaluating and predicting, *we must pass pairs of matrices*!!!\n",
    "\n",
    "Note we are splitting and overlapping on **features** which means features 2-4 go through both the deep and wide paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a3bcd77",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "162/162 [==============================] - 0s 1ms/step - loss: nan\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"sgd\") # Compile the model as usual\n",
    "\n",
    "# Split all feature matrices into A and B sets\n",
    "# Note we are sending all observations (instances)\n",
    "# Note we are SPLITTING FEATURES, and the split overlaps on features 2-5\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "\n",
    "# Split the split feature test sets into \"new\" instances\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "# Fit model and store as variable to inspect history object\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    " validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "\n",
    "# Eval\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "# Predict\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357051e7",
   "metadata": {},
   "source": [
    "There are also many cases where you may want **multiple outputs**:\n",
    "\n",
    "- The task may demand it. What if you want to locate AND classify an object in a picture. This is a classification task and regression task. We must find the coordinates to the objects center, and width and height, and classify the object.\n",
    "\n",
    "- Multiple outputs can help with multiple independent tasks that use the same data. You could do one neural network per task, but multi-output neural networks will learn features that are useful across tasks better.\n",
    "\n",
    "- Another use is regularization (constraining to reduce overfitting). For example, you may want to add some auxiliary outputs in a neural network architecture (see Figure 10-15, p308) to ensure that the underlying part of the network learns something useful on its own, without relying on the rest of the network.\n",
    "\n",
    "**Adding extra outputs** is to simply connect extra output to the appropriate layers and add it to the list of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e58851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same inputs and hidden layers as the block above\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "aux_output = keras.layers.Dense(1)(hidden2)\n",
    "model = keras.models.Model(inputs=[input_A, input_B],\n",
    "                           outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c2d47",
   "metadata": {},
   "source": [
    "For compilation, each output needs it own loss or it will compute the same loss for every output. loss weights should also be specified as we should care more about the main output than the auxilary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78519f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c5b0bd",
   "metadata": {},
   "source": [
    "Since we have multiple outputs, we need to train (fit) the model with labels for each output. In this case, the labels are the same so we will use the list [y_train,y_train]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "591e5ff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 3.5647 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 2.3512\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.8494 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.5693\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.4471 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3774\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3524 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3300\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3305 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3176\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3255 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3142\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3244 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3131\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3126\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3126\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3124\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3124\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3124\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3122\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train],\n",
    "                    epochs=20, \n",
    "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e497af1",
   "metadata": {},
   "source": [
    "Evaluating returns the total and individual losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b8a0712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3633\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B],\n",
    "                                                 [y_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f0eaeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "1.3633192777633667\n"
     ]
    }
   ],
   "source": [
    "print(total_loss)\n",
    "print(main_loss)\n",
    "print(aux_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0458ad5",
   "metadata": {},
   "source": [
    "Predict returns predictions for each output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b25c7acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e80859e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "[[2.0608408]\n",
      " [2.0608408]\n",
      " [2.0608408]]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred_main)\n",
    "print(y_pred_aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66d3d1f",
   "metadata": {},
   "source": [
    "You can build whatever architecture you want with the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb5d94d",
   "metadata": {},
   "source": [
    "### Building Dynamic Models Using the Subclassing API\n",
    "The Sequential and Functional APIs are declarative: you declare the layers and how they should connect, then feed the model some data for training or inference. The advantages are you can save, clone, and share the model, its structure can be displayed and analyzed, the framework can infer shapes and check types, so debugging is easy. The problem is its static.\n",
    "\n",
    "The **Subclassing API** allows dealing with loops, differing shapes, conditional branching, and other dynamic behaviors. This is the imperative style.\n",
    "\n",
    "How to use the Subclassing API? Subclass the *Model* class, create your layers in the constructor, then perform computations in the `call` method.\n",
    "\n",
    "The below model is equivalent to the one using the Functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c44f1d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.models.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs) # handles standard args (e.g., name)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "\n",
    "# model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d68c22e",
   "metadata": {},
   "source": [
    "Note that *we never created input layers*. We just use the input argument in the call method. Other than that and separating the creating from connecting the layers, this is the same as the Functional API example.\n",
    "\n",
    "The biggest difference between the Functional and Subclassing APIs are that we *can do anything we want in the call method*: for loops, if statements, etc, anything your heart desires!\n",
    "\n",
    "This extra flexibility costs the ability to inspect (with summary), save, and clone the model. So unless you need the extra flexibility, stick with the Sequential or Functional APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb61c8c",
   "metadata": {},
   "source": [
    "### Saving and Restoring a Model\n",
    "Saving a model is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9489006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b1e46",
   "metadata": {},
   "source": [
    "Keras saves the model's architecture (including every layer's hyperparameters) and the value of all model parameters for every layer being the connection weights and biases, all in the HDF5 format (.h5). It also saves the optimizer and any state it might have. \n",
    "\n",
    "You should typically have a script that trains a model, saves it, then loads it. Loading is just as easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92a9256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2568ec8a",
   "metadata": {},
   "source": [
    "Recall: saving works for Sequential and Functional APIs, but not Subclassing API.\n",
    "\n",
    "With the Subclassing API, you can use the save_weights() and load_weights() methods, but nothing else is saved.\n",
    "\n",
    "**What if training lasts several hours**? This is very common on large datasets. We want to make checkpoints in training (fitting), just in case. You can use **callbacks** with the .fit() method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ba1ab7",
   "metadata": {},
   "source": [
    "### Using Callbacks\n",
    "The `.fit()` method accepts a `callbacks` argument that **specifies a list of objects to call** at the start and end of training, the start and end of each epoch, and the start and end of each batch.\n",
    "\n",
    "The `ModelCheckpoint` callback saves chechpoints at regular intervals during training, by default, at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "965955d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3122\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3122\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3124\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3124\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3124\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3122\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3122\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3122\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3122\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3124\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n"
     ]
    }
   ],
   "source": [
    "# Build and compile a model\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\")\n",
    "\n",
    "# Then train the model with the callbacks argument\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    " validation_data=((X_valid_A, X_valid_B), y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eced16",
   "metadata": {},
   "source": [
    "**Keras has \"early stopping\" built-in**: the `save_best_only = True`, when used with a validation set only saves the on validation accuracies that are better than before; this prevents overfitting and running for too long.\n",
    "\n",
    "Restore the last model saved after training and it will be the one that performed best on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e00df8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3122\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3124\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3124\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3124\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3122\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3122\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3122\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3124\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3122\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",\n",
    "                                                save_best_only=True) # True for early stopping\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), \n",
    "                    y_train, epochs=20,\n",
    "                    validation_data=((X_valid_A, X_valid_B), \n",
    "                                     y_valid), # Validation data is needed for above\n",
    "                    callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model(\"my_keras_model.h5\") # rollback to best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97c00e3",
   "metadata": {},
   "source": [
    "Another way is with the `EarlyStopping` callback. It interrupts when it measures no progress on the validation set for a number of epochs (defined by a `patience` argument) and optionally rolls back to the best model with `restore_best_weights = True`. \n",
    "\n",
    "Combining both callbacks is smart in case of a computer crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a4954a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3122\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3122\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3121\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, # Epochs without progress\n",
    "                                                  restore_best_weights=True) # rollback model\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    " validation_data=((X_valid_A, X_valid_B), y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f00bb",
   "metadata": {},
   "source": [
    "Many more callbacks available at https://keras.io/callbacks/\n",
    "\n",
    "We can also write our own callbacks: the following displays the ratio between training loss and validation loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caaef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b97e4b",
   "metadata": {},
   "source": [
    "Note the function definition `on_epoch_end`. As suspected, we can use:\n",
    "- on_epoch_begin()\n",
    "- on_train_begin()/_end()\n",
    "- on_batch_begin()/_end()\n",
    "\n",
    "Callbacks are usable for evaluation and prediction, called by evaluate() and predict(), respectively.\n",
    "- on_test_begin()/_end()\n",
    "- on_test_batch_begin()/_end()\n",
    "\n",
    "- on_predict_begin()/_end()\n",
    "- on_predict_batch_begin()/_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c0711d",
   "metadata": {},
   "source": [
    "### Visualization Using Tensorboard\n",
    "Tensorboard is an interactive viz tool. It is already installed with Tensorflow.\n",
    "\n",
    "It allows:\n",
    " - view learning curves during training\n",
    " - compare learning curves between runs\n",
    " - visualize the computation graph\n",
    " - analyze training statistics\n",
    " - view images generated by the model\n",
    " - viz complex n-D data down to 3D\n",
    " - and more!!\n",
    " \n",
    "To use: modify program to output special binary log files called *event files*. Each binary data record is called a *summary*... \n",
    "\n",
    "We need to create a \"log\" directory and connect the Tensorboard to it. Then, Tensorboard will automatically update when event files in the log directory change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3df1ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a5a6529",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logdir = os.path.join(os.curdir, \"my_logs\") # Create the special binary log directory\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_01_16-11_28_43'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58399f08",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3124\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3122\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3122\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3122\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3124\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3124\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3124\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3124\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3124\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3241 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: nan - dense_14_loss: nan - dense_15_loss: 1.3240 - val_loss: nan - val_dense_14_loss: nan - val_dense_15_loss: 1.3123\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    " validation_data=((X_valid_A, X_valid_B), y_valid),\n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a4cff4",
   "metadata": {},
   "source": [
    "Because I use a Jupyter notebook:\n",
    " - Go to anaconda prompt\n",
    " - cd until at the directory with `my_logs` directory\n",
    " - run command: tensorboard --logdir=./my_logs --port=6006\n",
    " - open up the local host given (http://localhost:6006)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295bd9e9",
   "metadata": {},
   "source": [
    "## Fine-Tuning Neural Network Hyperparameters\n",
    "The flexibility comes with some drawbacks, one being there are many hyperparameters.\n",
    "\n",
    "We learned that we can create any type of network architecture, and even in a simple MLP we still can modify the number of layers, neurons per layer, activation functions, weight initializations, and more.\n",
    "\n",
    "One option is to tune parameters manually, or we can use k-fold CV using GridSearchCV or RandomizedSearchCV which is likely the better option.\n",
    "\n",
    "**To use k-fold CV on our Keras models, we need to wrap the model in objects that mimic SKlearn regressors.**\n",
    "\n",
    "1. Create a function that builds and compiles a Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3000e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapped Sequential model\n",
    "\n",
    "\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    options = {\"input_shape\": input_shape}\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\", **options))\n",
    "        options = {}\n",
    "    model.add(keras.layers.Dense(1, **options))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c3454d",
   "metadata": {},
   "source": [
    "2. Create a Keras wrapper based on the model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "68750dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3594c2",
   "metadata": {},
   "source": [
    "`KerasRegressor` is a thin wrapper around our model. It doesnt add anything (since we didn't tell it to), and just uses the parameters set in `build_model()`\n",
    "\n",
    "3. Now we can use the object like a regular SKlearn regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a905da11",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: inf - val_loss: 5579897461274348970077855940608.0000\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1269171731140759508655777251328.0000 - val_loss: 70654606994918356445529374720.0000\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 16070724337377173301301870592.0000 - val_loss: 894655576806611258615791616.0000\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 203493504808439916244500480.0000 - val_loss: 11328500635488454556254208.0000\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2576717305035054221623296.0000 - val_loss: 143445421746472586575872.0000\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 32627349817600971374592.0000 - val_loss: 1816361824626048761856.0000\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 928us/step - loss: 413140033985390313472.0000 - val_loss: 22999489671818379264.0000\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 5231332288934969344.0000 - val_loss: 291227782287982592.0000\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 924us/step - loss: 66241160347123712.0000 - val_loss: 3687635298025472.0000\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 946us/step - loss: 838770001707008.0000 - val_loss: 46694293045248.0000\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 987us/step - loss: 10620801908736.0000 - val_loss: 591259303936.0000\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 134484566016.0000 - val_loss: 7486760448.0000\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1702895744.0000 - val_loss: 94800440.0000\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 909us/step - loss: 21562666.0000 - val_loss: 1200441.2500\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 987us/step - loss: 273031.6875 - val_loss: 15206.4004\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 962us/step - loss: 3458.4417 - val_loss: 194.4865\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 45.0956 - val_loss: 3.8238\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 948us/step - loss: 1.8760 - val_loss: 1.3480\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3305 - val_loss: 1.3138\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3243 - val_loss: 1.3130\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 985us/step - loss: 1.3242 - val_loss: 1.3122\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3241 - val_loss: 1.3119\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3242 - val_loss: 1.3119\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 987us/step - loss: 1.3242 - val_loss: 1.3121\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3242 - val_loss: 1.3124\n",
      "Epoch 26/100\n",
      " 67/363 [====>.........................] - ETA: 0s - loss: 1.3908"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [62]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mkeras_reg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m              \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m mse_test \u001b[38;5;241m=\u001b[39m keras_reg\u001b[38;5;241m.\u001b[39mscore(X_test, y_test)\n\u001b[0;32m      5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m keras_reg\u001b[38;5;241m.\u001b[39mpredict(X_new)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py:162\u001b[0m, in \u001b[0;36mBaseWrapper.fit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m fit_args \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_sk_params(Sequential\u001b[38;5;241m.\u001b[39mfit))\n\u001b[0;32m    160\u001b[0m fit_args\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m--> 162\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(x, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_args)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1178\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1179\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1180\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1181\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1182\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1183\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1184\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1186\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    915\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    920\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    921\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[0;32m   3038\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1961\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1962\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1964\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1965\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m     args,\n\u001b[0;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1968\u001b[0m     executing_eagerly)\n\u001b[0;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a919f07c",
   "metadata": {},
   "source": [
    "Actually, we shouldnt train using the code above, let's use k-fold CV in training it.\n",
    "\n",
    "Note: Since there are many hyperparameters, it's better to use RandomizedSearch over GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e55d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_distribs = {\n",
    " \"n_hidden\": [0, 1, 2, 3],\n",
    " \"n_neurons\": np.arange(1, 100),\n",
    " \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1e2a85",
   "metadata": {},
   "source": [
    "This is identical to what we did in Chapter 2, but with extra arguments to the .fit() method.\n",
    "\n",
    "Exploration could last many hours depending on the size of the data, complexity of the model, hardware, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3023e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2143d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnd_search_cv.best_estimator_.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0045476c",
   "metadata": {},
   "source": [
    "There are many libraries to use to optimize hyperparameters\n",
    "- Scikit-Optimize\n",
    "- Spearmint\n",
    "- Sklearn-Deap\n",
    "- etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
