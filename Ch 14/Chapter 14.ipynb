{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95d29db8",
   "metadata": {},
   "source": [
    "# Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
    "\n",
    "Note: CNNs are not restricted to visual perception tasks and are useful in voice recognition and NLP tasks.\n",
    "\n",
    "In this chapter:\n",
    " - CNN theory and building blocks\n",
    " - implementation with TF and Keras\n",
    " - tasks like object detection and semantic segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e728ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_images\n",
    "import tensorflow as tf\n",
    "\n",
    "# Loading images, rescaling etc\n",
    "images = load_sample_images()[\"images\"]\n",
    "images = tf.keras.layers.CenterCrop(height=70, width=120)(images)\n",
    "images = tf.keras.layers.Rescaling(scale=1 / 255)(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa5b9e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 70, 120, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 images, 70x120 (bc of rescaling above), 3 - RGB (color channels)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d473bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7)\n",
    "# Apply convolution to the images\n",
    "fmaps = conv_layer(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "001ab672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64, 114, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same two images, applied with conv2D, no padding\n",
    "\n",
    "# New dims = 2 images, 64x114 after conv2D, 32 is feature intensity\n",
    "fmaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a533797d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 70, 120, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Padding = 'valid' means no zero padding, only 'valid' positions\n",
    "\n",
    "# Same conv_layer but with zero-padding\n",
    "conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7,\n",
    "                                    padding=\"same\")\n",
    "\n",
    "fmaps = conv_layer(images)\n",
    "fmaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2b9a8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If stride > 1, output size != input size\n",
    "\n",
    "# convolutional layers hold weights and biases as usual\n",
    "kernels, biases = conv_layer.get_weights()\n",
    "\n",
    "kernels.shape #  [kernel_height, kernel_width, input_channels, output_channels]\n",
    "\n",
    "biases.shape # [output_channels]\n",
    "\n",
    "# number of output channels == number output feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8287788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool = tf.keras.layers.MaxPool2D(pool_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1de9cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthPool(tf.keras.layers.Layer):\n",
    "    def __init__(self, pool_size=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)  # shape[-1] is the number of channels\n",
    "        groups = shape[-1] // self.pool_size  # number of channel groups\n",
    "        new_shape = tf.concat([shape[:-1], [groups, self.pool_size]], axis=0)\n",
    "        return tf.reduce_max(tf.reshape(inputs, new_shape), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fb247f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_avg_pool = tf.keras.layers.GlobalAvgPool2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e5036f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0.64338624, 0.5971759 , 0.5824972 ],\n",
       "       [0.76306933, 0.26011038, 0.10849128]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_avg_pool(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f954a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding=\"same\",\n",
    "                        activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "model = tf.keras.Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2D(filters=128),\n",
    "    DefaultConv2D(filters=128),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    DefaultConv2D(filters=256),\n",
    "    DefaultConv2D(filters=256),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=128, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=64, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226a4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
