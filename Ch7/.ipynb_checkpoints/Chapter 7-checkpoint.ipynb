{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebeb464f",
   "metadata": {},
   "source": [
    "# Chapter 7: Ensemble Learning and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03d326f",
   "metadata": {},
   "source": [
    "Asking a question to a crowd of thousands of people and aggregating the answer will likely give you a better answer than asking an expert. This is called *wisdom of the crowd*. \n",
    "\n",
    "Similarly, we can aggregate the predictions of a group of predictors (predictors in this case are models: classifiers or regressors) and we get better predictions with a group than with even the best individual predictor. \n",
    "\n",
    "A group of predictors is called an **Ensemble**, thus we have **Ensemble Learning**. An Ensemble Learning algorithm is called an **Ensemble method**.\n",
    "\n",
    "For example, we can train a group of Decision Trees on different subsets of a training set, make predictions, then select the class which gets the most \"votes\" from the group of trees. An ensemble of Decision Trees is called a **Random Forest** and is one of the most powerful ML algorithms available; Ensemble methods frequently win ML competitions.\n",
    "\n",
    "This chapter covers:\n",
    " - Bagging and Pasting\n",
    " - Boosting\n",
    " - Stacking\n",
    " - etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688c59cb",
   "metadata": {},
   "source": [
    "## Voting Classifiers\n",
    "Suppose we have some different classification predictors (models), each one achieving around 80% accuracy: LRC, SVM, RFC (random forest), KNN, etc. \n",
    "\n",
    "A simple way to get an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. A majority voting classifier is called a *Hard Voting* classifier.\n",
    "\n",
    "The voting classifier will produce a higher accuracy than the individual classifiers, themselves. \n",
    "\n",
    "Even if our voting classifier is filled with *weak learners* (algorithms that are only slightly better than random guessing), the ensemble is usually still better than a strong learner, provided there are a lot of weak learners and they are diverse. *Ensemble methods work best when the predictors are as independent as possible from one another*.\n",
    "\n",
    "Below see creation of a voting classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff867fcf",
   "metadata": {},
   "source": [
    "```\n",
    "# Imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 3 Different (and diverse) classifiers\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "\n",
    "# Ensemble method: VotingClassifier\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), \n",
    "                                          ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "                              voting='hard')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7fd14d",
   "metadata": {},
   "source": [
    "When running this code, the `voting_clf` should at least slightly outperform the individual classifiers.\n",
    "\n",
    "If all classifiers are able to estimate class probabilites (have a predict_proba() method), then SciKit-Learn can predict the class with the highest probability, averaged over the individual classifiers. This is called **soft voting**. Soft voting **achieves higher accuracy than hard voting because it gives higher weight to highly confident votes**.\n",
    "\n",
    "To *use soft voting*, replace `voting = 'hard'` with `voting = 'soft'`\n",
    "\n",
    "Note: SVC does not output probabilities by default; it needs the hyperparameter `probability = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b24942b",
   "metadata": {},
   "source": [
    "## Bagging and Pasting\n",
    "With Voting Classifiers, we used diverse (very different) training algorithms (Support vector, Logistic regression, and random forest in the above code).\n",
    "\n",
    "Another approach is to use many of the same training algorithm (predictor), but on different subsets of the training set. \n",
    "\n",
    "**Bagging**, short for *Bootstrap aggregating*, is taking subsets of the training set *with replacement*.\n",
    "\n",
    "**Pasting** is taking subsets of the training set *without replacement*.\n",
    "\n",
    "**Bagging and Pasting allow training instances to be sampled several times over multiple predictors, but ONLY BAGGING allows training instances to be samples several times for the same predictor, hence with replacement**.\n",
    "\n",
    "Once all predictors are trained, the ensemble makes the prediction by aggregating the individual predictions. The aggregation function is usually the *statistical mode* (the most frequent prediction just like Hard Voting), or the average for a Regression task.\n",
    "\n",
    "The individual predictors have a higher bias than if trained on the full training set, but the ensemble will have a similar bias and a lower variance.\n",
    "\n",
    "Training and predictions in ensemble methods can be done in parallel by allocating CPU cores or using different servers; just another reason ensembles are so strong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca561e09",
   "metadata": {},
   "source": [
    "### Bagging and Pasting in Scikit-Learn\n",
    "Bagging and Pasting are achieved by using the same class in SK-Learn.\n",
    "\n",
    "```\n",
    "# Imports\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Bagging/Pasting class\n",
    "bag_clf = BaggingClassifier(\n",
    " DecisionTreeClassifier(), n_estimators=500,\n",
    " max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "```\n",
    "\n",
    "In the above code for the BaggingClassifier():\n",
    " - DecisionTreeClassifier() is the predictor\n",
    " - `n_estimators` is using the predictor (Decision Tree) 500 times\n",
    " - `max_samples = 100` means each of the 500 predictors will be trained on 100 training instances\n",
    " - `bootstrapping = True` means bagging is used\n",
    "  - `= False` means pasting is used\n",
    " - `n_jobs = -1` means SK-Learn will use all CPU cores!\n",
    " \n",
    "BaggingClassifier *automatically performs soft voting if the base classifier has a predict_proba() method*.\n",
    "\n",
    "Note: Bagging generally performs better, but checking pasting with CV is a good idea sometimes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f1a338",
   "metadata": {},
   "source": [
    "### Out of Bag Evaluation\n",
    "With bagging, since bootstrapping is used, some training instances are never seen by any classifier in the ensemble; these samples are said to be **out-of-bag (oob) instances**; because the *out-of-bag* instances were never seen, they can be used for evaluation on the predictor instead of a validation set.\n",
    "\n",
    "With the BaggingClassifier() use the `oob_score = True` parameter to automatically evaluate on the out-of-bag instances after training. The oob_score is recoverable with the `.oob_score_` attribute.\n",
    "\n",
    "```\n",
    " # Note the oob_score=True parameter\n",
    " bag_clf = BaggingClassifier(DecisionTreeClassifier(), \n",
    "                             n_estimators=500, bootstrap=True, \n",
    "                             n_jobs=-1, oob_score=True)\n",
    "\n",
    " bag_clf.fit(X_train, y_train)    # Fitting automatically evals the oob instances\n",
    " bag_clf.oob_score_               # Evaluation score on oob instances\n",
    "```\n",
    "\n",
    "We can also get the prediction probabilities with the `oob_decision_function_ ` attribute.\n",
    "\n",
    "```\n",
    " bag_clf.oob_decision_function_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9ba319",
   "metadata": {},
   "source": [
    "## Random Patches and Random Subspaces\n",
    "BaggingClassifier also support sampling features as well as the instances.\n",
    "\n",
    "Sampling the features is controlled by the two hyperparameter `max_features` and `bootstrap_features`, which work exactly the same way as sampling instances, just with the features instead. Thus, each predictor (model) is trained on a random subset of the input features.\n",
    "\n",
    "This is particularly useful when dealing with high-dimensional inputs, like images.\n",
    "\n",
    "Sampling both instances and features is called the **Random Patches method**.\n",
    "\n",
    "Keeping all training instances (for all predictors in the ensemble? ; with `bootstrap = False` and `max_samples = 1.0`) but sampling features (`bootstrap_features = True` and/or `max_features` < 1.0) is called **Random Subspaces method**.\n",
    "\n",
    "Note: using max_sample = 1.0 is saying to sample 100% of the instances for each predictor. Pretty sure.\n",
    "\n",
    "Sampling features results in even more predictor diversity, trading a bit more bias for\n",
    "a lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82863fc4",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "Recall a Random Forest is an Ensemble of Decision Trees. Random Forests generally use bagging, but sometimes pasting. \n",
    "\n",
    "The RandomForestClassifier() (or RandomForestRegressor) is optimized for Decision Trees and better to use over the BaggingClassifier(). \n",
    "\n",
    "The following code trains 500 Decision trees with a maximum of 16 nodes per tree.\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "```\n",
    "\n",
    "With a few exceptions, the RandomForestClassifier has all the hyperparameters of the DecisionTreeClassifier to control how the tree grows, and BaggingClassifier to control the ensemble itself.\n",
    "\n",
    "The Random Forest, instead of searching for THE very best feature when splitting a node, it searches for the best feature in a random subset of nodes. This results in greater tree diversity, a higher bias, lower variance, and a generally better model.\n",
    "\n",
    "The code below is \"roughly equivalent to a RandomForestClassifier\":\n",
    "\n",
    "```\n",
    "bag_clf = BaggingClassifier(\n",
    " DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    " n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)\n",
    "```\n",
    "\n",
    "### Extra Trees\n",
    "Note, in the paragraph above that the Random Forest searches for the best feature in a random subset of features: this is done by searching for the best threshold for splitting. We can make the Random Forest even more random by making the threshold that it searches to split on, randomized. This is called an **Extremely Randomized Tree Ensemble** aka **\"Extra Trees\"**.\n",
    "\n",
    "Once again, this increases bias, but reduces variance. **Training is much faster than random forests** because searching for the best threshold is one of the most time-consuming tasks; making the threshold random takes way less time. \n",
    "\n",
    "The `ExtraTreesClassifier` and `ExtraTreesRegressor` has identical API to the RandomForest equivalents and will use the same hyperparameters.\n",
    "\n",
    "*It is difficult to know whether ExtraTrees or RandomForest performs better on a problem without using GridSeachCV.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b13d88",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "Another great quality of Random Forests is that SK-learn measures automatically measures a feature's importance by taking the average impurity reduction across all nodes that use that feature. It is a weighted average where the node's weight depends on the number of training samples associated with it.\n",
    "\n",
    "The `feature_importances_` attribute of the Random Forest will sum to 1.\n",
    "\n",
    "Below is an example on the iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c1da961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.1034164636603887\n",
      "sepal width (cm) 0.024645340382488886\n",
      "petal length (cm) 0.4379600550708183\n",
      "petal width (cm) 0.43397814088630415\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfcd4e3",
   "metadata": {},
   "source": [
    "Note that the petal length and width are seen as the most important features in predicting flowers.\n",
    "\n",
    "Checking feature importances is also useful to see which pixels in a picture are the most important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4c00f7",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "Boosting (originally Hypothesis Boosting) refers to any Ensemble method that combines several weak learners into a strong learner.\n",
    "\n",
    "The general idea of most boosting methods is to train predictors (models) sequentially, each trying to correct its predecessor.\n",
    "\n",
    "There are many boosting methods, but the two popular are: AdaBoost (Adaptive Boosting) and Gradient Boosting.\n",
    "\n",
    "### Adaboost\n",
    "Adaboosts correction technique is to have each new predictor (model algorithm) correct its predecessor by paying a bit more attention on the underfit training instances. The new predictors focus more and more on the hard cases.\n",
    "\n",
    "Adaboosting algorithm:\n",
    "1. Train a base classifier and make predictions on the training set. For all misclassified training instances, their weights are increased. \n",
    "2. A subsequent classifier trains on the training set with the updated instance weights; again predicting on the training set and increasing weights for all misclassified training instances\n",
    "3. Repeat step 2 until going through all predictors.\n",
    "4. Upon training all predictors, predictors with the highest accuracy have the most say, a higher weight, when predicting an instance.\n",
    "\n",
    "A worthy note: The sequential architecture of some boosting techniques makes for a slow computation time because predictors cannot be trained in parallel, as $predictor_j$ depends on the updated weights of $predictor_i$, and $predictor_k$ depends on $predictor_j$ and $predictor_i$, and so on.\n",
    "\n",
    "See pages 201-203 in Geron for the mathematics of updating weights.\n",
    "\n",
    "SK-Learn uses a multiclass version of Adaboost called \"SAMME\", unless the predictors in the boosting sequence have a predict_proba() method and can output probabilities, then it uses \"SAMME.R\" which frequently performs better.\n",
    "\n",
    "An example of AdaBoostClassifier():\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    " DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    " algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    " \n",
    "ada_clf.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "The above Adaboost algorithm's predictors are *Decision Stumps*.\n",
    "\n",
    "*Decision Stumps* are decision trees of `max_depth = 1`, which only contain depth0 and depth 1, a root node and its two children.\n",
    "\n",
    "If the Adaboost ensemble is overfitting, reduce the number of predictors with `n_estimators` or constrain (regularize) the base predictor itself.\n",
    "\n",
    "\n",
    "### Gradient Boosting\n",
    "The Gradient Boosting ensemble has the same sequential architecture as the Adaboost ensemble that looks to correct its predecessor, but instead of updating instance weights, the Gradient Boost ensemble fits on the residual errors (error for each instance) generated by the preceeding predictor.\n",
    "\n",
    "Gradient Boosting works with Regression tasks, so we will boost DecisionTreeRegressors. This has a name in itself, Gradient Boosted Regression Trees.\n",
    "\n",
    "The *following code exhibits Gradient Boosting without using the Gradient Boosting ensemble class*:\n",
    "\n",
    "First create a Decision Tree Regressor\n",
    "```\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X, y)\n",
    "```\n",
    "\n",
    "Train another Decision Tree Regressor on the prediction errors of the first tree\n",
    "\n",
    "```\n",
    "y2 = y - tree_reg1.predict(X) # Errors of first tree\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2) # Fit with errors of first tree\n",
    "```\n",
    "\n",
    "Train a 3rd Decision Tree Regressor on the prediction errors of the second tree\n",
    "\n",
    "```\n",
    "y3 = y2 - tree_reg2.predict(X) # Errors of second tree\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3) # Fit with errors of second tree\n",
    "```\n",
    "and so on,\n",
    "\n",
    "The above 3 code blocks is the ensemble, and it predicts by adding the predictions for all trees\n",
    "\n",
    "```\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "```\n",
    "\n",
    "Equivalently we can use the SK-Learn GBRT to achieve the same results:\n",
    "\n",
    "```\n",
    "# This does the exactly the same as previous 4 code blocks\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X, y)\n",
    "```\n",
    "\n",
    "Similar to RandomForestRegressor, the GradientBoostingRegressor's hyperparameters control tree-growth and ensemble training.\n",
    "\n",
    "\"The learning_rate hyperparameter scales the contribution of each tree.\" A low learning rate means more trees in the ensemble, as each tree will have a smaller contribution, but how do we actually find the optimal number of trees in the Boosting sequence?\n",
    "\n",
    "The Gradient Boost implementation has a staged_predict() method that **supports finding the optimal number of trees**.\n",
    "\n",
    "The stage_predict() method evaluates the prediction error on a validation set at each iteration stage of training (train the first tree, eval on first; train the second tree; eval on first and second, and so on)(evaluates on 1 tree, 2 trees, 3 trees, etc). \n",
    "\n",
    "The following code finds the optimal number of trees:\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "# Create a GBRT with 120 trees\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "# Now, the MSE is evaluated for each set of 1,2,3,...,120 trees\n",
    "# The 120th (119 in python) index of the errors list will have its prediction evaluated \n",
    "# by all 120 trees\n",
    "# While the first (0th) index only evaluated on the first tree.\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    " for y_pred in gbrt.staged_predict(X_val)]\n",
    " \n",
    "# Take the argmin is finding the index, also the number, of trees that gives the lowest MSE\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "\n",
    "# recreate a GRBT with the best number of estimators, bst_n_estimators\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Or we can implement early stopping with the `warm_start=True` parameter to the GBRT class:\n",
    "\n",
    "\"The following code stops training when the validation error does not improve for five iterations in a row\"\n",
    "\n",
    "```\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    " gbrt.n_estimators = n_estimators\n",
    " gbrt.fit(X_train, y_train)\n",
    " y_pred = gbrt.predict(X_val)\n",
    " val_error = mean_squared_error(y_val, y_pred)\n",
    " if val_error < min_val_error:\n",
    " min_val_error = val_error\n",
    " error_going_up = 0\n",
    " else:\n",
    " error_going_up += 1\n",
    " if error_going_up == 5:\n",
    " break # early stopping\n",
    " \n",
    "```\n",
    "\n",
    "The GradientBoostRegressor also has a `subsample = ` hyperparameter that accepts 0 to 1.0 and is a fraction of training instances to be used for each predictor in the sequence. The training instances are selected randomly, and as usual, this increases the bias but decreases the variance in the ensemble. This is also called **Stochastic Gradient Boosting**.\n",
    "\n",
    "Gradient boosting can also be done with different cost functions with the `loss` hyperparameter (see SK-Learn docs)\n",
    "\n",
    "### XGBoost - Extreme Gradient Boosting\n",
    "An optimized implementation of Gradient Boosting.\n",
    "\n",
    "```\n",
    "import xgboost\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)\n",
    "\n",
    "xgb_reg.fit(X_train, y_train,\n",
    " eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
    "y_pred = xgb_reg.predict(X_val)\n",
    "```\n",
    "\n",
    "\"XGBoosts API is similar to SK-Learn's\", and the book says to \"check it out\". (This is all that was given in the chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41a4e6e",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\"The last ensemble method discussed in this chapter is Stacking (short for Stacked Generalization)\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
