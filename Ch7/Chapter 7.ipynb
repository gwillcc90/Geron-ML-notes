{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebeb464f",
   "metadata": {},
   "source": [
    "# Chapter 7: Ensemble Learning and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03d326f",
   "metadata": {},
   "source": [
    "Asking a question to a crowd of thousands of people and aggregating the answer will likely give you a better answer than asking an expert. This is called *wisdom of the crowd*. \n",
    "\n",
    "Similarly, we can aggregate the predictions of a group of predictors (predictors in this case are models: classifiers or regressors) and we get better predictions with a group than with even the best individual predictor. \n",
    "\n",
    "A group of predictors is called an **Ensemble**, thus we have **Ensemble Learning**. An Ensemble Learning algorithm is called an **Ensemble method**.\n",
    "\n",
    "For example, we can train a group of Decision Trees on different subsets of a training set, make predictions, then select the class which gets the most \"votes\" from the group of trees. An ensemble of Decision Trees is called a **Random Forest** and is one of the most powerful ML algorithms available; Ensemble methods frequently win ML competitions.\n",
    "\n",
    "This chapter covers:\n",
    " - Bagging and Pasting\n",
    " - Boosting\n",
    " - Stacking\n",
    " - etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688c59cb",
   "metadata": {},
   "source": [
    "## Voting Classifiers\n",
    "Suppose we have some different classification predictors (models), each one achieving around 80% accuracy: LRC, SVM, RFC (random forest), KNN, etc. \n",
    "\n",
    "A simple way to get an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. A majority voting classifier is called a *Hard Voting* classifier.\n",
    "\n",
    "The voting classifier will produce a higher accuracy than the individual classifiers, themselves. \n",
    "\n",
    "Even if our voting classifier is filled with *weak learners* (algorithms that are only slightly better than random guessing), the ensemble is usually still better than a strong learner, provided there are a lot of weak learners and they are diverse. *Ensemble methods work best when the predictors are as independent as possible from one another*.\n",
    "\n",
    "Below see creation of a voting classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff867fcf",
   "metadata": {},
   "source": [
    "```\n",
    "# Imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 3 Different (and diverse) classifiers\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "\n",
    "# Ensemble method: VotingClassifier\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), \n",
    "                                          ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "                              voting='hard')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7fd14d",
   "metadata": {},
   "source": [
    "When running this code, the `voting_clf` should at least slightly outperform the individual classifiers.\n",
    "\n",
    "If all classifiers are able to estimate class probabilites (have a predict_proba() method), then SciKit-Learn can predict the class with the highest probability, averaged over the individual classifiers. This is called **soft voting**. Soft voting **achieves higher accuracy than hard voting because it gives higher weight to highly confident votes**.\n",
    "\n",
    "To *use soft voting*, replace `voting = 'hard'` with `voting = 'soft'`\n",
    "\n",
    "Note: SVC does not output probabilities by default; it needs the hyperparameter `probability = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b24942b",
   "metadata": {},
   "source": [
    "## Bagging and Pasting\n",
    "With Voting Classifiers, we used diverse (very different) training algorithms (Support vector, Logistic regression, and random forest in the above code).\n",
    "\n",
    "Another approach is to use many of the same training algorithm (predictor), but on different subsets of the training set. \n",
    "\n",
    "**Bagging**, short for *Bootstrap aggregating*, is taking subsets of the training set *with replacement*.\n",
    "\n",
    "**Pasting** is taking subsets of the training set *without replacement*.\n",
    "\n",
    "**Bagging and Pasting allow training instances to be sampled several times over multiple predictors, but ONLY BAGGING allows training instances to be samples several times for the same predictor, hence with replacement**.\n",
    "\n",
    "Once all predictors are trained, the ensemble makes the prediction by aggregating the individual predictions. The aggregation function is usually the *statistical mode* (the most frequent prediction just like Hard Voting), or the average for a Regression task.\n",
    "\n",
    "The individual predictors have a higher bias than if trained on the full training set, but the ensemble will have a similar bias and a lower variance.\n",
    "\n",
    "Training and predictions in ensemble methods can be done in parallel by allocating CPU cores or using different servers; just another reason ensembles are so strong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca561e09",
   "metadata": {},
   "source": [
    "### Bagging and Pasting in Scikit-Learn\n",
    "Bagging and Pasting are achieved by using the same class in SK-Learn.\n",
    "\n",
    "```\n",
    "# Imports\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Bagging/Pasting class\n",
    "bag_clf = BaggingClassifier(\n",
    " DecisionTreeClassifier(), n_estimators=500,\n",
    " max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "```\n",
    "\n",
    "In the above code for the BaggingClassifier():\n",
    " - DecisionTreeClassifier() is the predictor\n",
    " - `n_estimators` is using the predictor (Decision Tree) 500 times\n",
    " - `max_samples = 100` means each of the 500 predictors will be trained on 100 training instances\n",
    " - `bootstrapping = True` means bagging is used\n",
    "  - `= False` means pasting is used\n",
    " - `n_jobs = -1` means SK-Learn will use all CPU cores!\n",
    " \n",
    "BaggingClassifier *automatically performs soft voting if the base classifier has a predict_proba() method*.\n",
    "\n",
    "Note: Bagging generally performs better, but checking pasting with CV is a good idea sometimes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4713c1cc",
   "metadata": {},
   "source": [
    "### Out of Bag Evaluation\n",
    "With bagging, since bootstrapping is used, some training instances are never seen by any classifier in the ensemble; these samples are said to be **out-of-bag (oob) instances**; because the *out-of-bag* instances were never seen, they can be used for evaluation on the predictor instead of a validation set.\n",
    "\n",
    "With the BaggingClassifier() use the `oob_score = True` parameter to automatically evaluate on the out-of-bag instances after training. The oob_score is recoverable with the `.oob_score_` attribute.\n",
    "\n",
    "```\n",
    " # Note the oob_score=True parameter\n",
    " bag_clf = BaggingClassifier(DecisionTreeClassifier(), \n",
    "                             n_estimators=500, bootstrap=True, \n",
    "                             n_jobs=-1, oob_score=True)\n",
    "\n",
    " bag_clf.fit(X_train, y_train)    # Fitting automatically evals the oob instances\n",
    " bag_clf.oob_score_               # Evaluation score on oob instances\n",
    "```\n",
    "\n",
    "We can also get the prediction probabilities with the `oob_decision_function_ ` attribute.\n",
    "\n",
    "```\n",
    " bag_clf.oob_decision_function_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8062ec9c",
   "metadata": {},
   "source": [
    "## Random Patches and Random Subspaces\n",
    "BaggingClassifier also support sampling features as well as the instances.\n",
    "\n",
    "Sampling the features is controlled by the two hyperparameter `max_features` and `bootstrap_features`, which work exactly the same way as sampling instances, just with the features instead. Thus, each predictor (model) is trained on a random subset of the input features.\n",
    "\n",
    "This is particularly useful when dealing with high-dimensional inputs, like images.\n",
    "\n",
    "Sampling both instances and features is called the **Random Patches method**.\n",
    "\n",
    "Keeping all training instances (for all predictors in the ensemble? ; with `bootstrap = False` and `max_samples = 1.0`) but sampling features (`bootstrap_features = True` and/or `max_features` < 1.0) is called **Random Subspaces method**.\n",
    "\n",
    "Note: using max_sample = 1.0 is saying to sample 100% of the instances for each predictor. Pretty sure.\n",
    "\n",
    "Sampling features results in even more predictor diversity, trading a bit more bias for\n",
    "a lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c72d6fd",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "Recall a Random Forest is an Ensemble of Decision Trees. Random Forests generally use bagging, but sometimes pasting. \n",
    "\n",
    "The RandomForestClassifier() (or RandomForestRegressor) is optimized for Decision Trees and better to use over the BaggingClassifier(). \n",
    "\n",
    "The following code trains 500 Decision trees with a maximum of 16 nodes per tree.\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "```\n",
    "\n",
    "With a few exceptions, the RandomForestClassifier has all the hyperparameters of the DecisionTreeClassifier to control how the tree grows, and BaggingClassifier to control the ensemble itself.\n",
    "\n",
    "The Random Forest, instead of searching for THE very best feature when splitting a node, it searches for the best feature in a random subset of nodes. This results in greater tree diversity, a higher bias, lower variance, and a generally better model.\n",
    "\n",
    "The code below is \"roughly equivalent to a RandomForestClassifier\":\n",
    "\n",
    "```\n",
    "bag_clf = BaggingClassifier(\n",
    " DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    " n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)\n",
    "```\n",
    "\n",
    "### Extra Trees\n",
    "Note, in the paragraph above that the Random Forest searches for the best feature in a random subset of features: this is done by searching for the best threshold for splitting. We can make the Random Forest even more random by making the threshold that it searches to split on, randomized. This is called an **Extremely Randomized Tree Ensemble** aka **\"Extra Trees\"**.\n",
    "\n",
    "Once again, this increases bias, but reduces variance. **Training is much faster than random forests** because searching for the best threshold is one of the most time-consuming tasks; making the threshold random takes way less time. \n",
    "\n",
    "The `ExtraTreesClassifier` and `ExtraTreesRegressor` has identical API to the RandomForest equivalents and will use the same hyperparameters.\n",
    "\n",
    "*It is difficult to know whether ExtraTrees or RandomForest performs better on a problem without using GridSeachCV.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6eee05",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "Another great quality of Random Forests is that SK-learn measures automatically measures a feature's importance by taking the average impurity reduction across all nodes that use that feature. It is a weighted average where the node's weight depends on the number of training samples associated with it.\n",
    "\n",
    "The `feature_importances_` attribute of the Random Forest will sum to 1.\n",
    "\n",
    "Below is an example on the iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b952c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_iris\n\u001b[0;32m      2\u001b[0m iris \u001b[38;5;241m=\u001b[39m load_iris()\n\u001b[1;32m----> 3\u001b[0m rnd_clf \u001b[38;5;241m=\u001b[39m \u001b[43mRandomForestClassifier\u001b[49m(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m rnd_clf\u001b[38;5;241m.\u001b[39mfit(iris[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m], iris[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(iris[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_names\u001b[39m\u001b[38;5;124m\"\u001b[39m], rnd_clf\u001b[38;5;241m.\u001b[39mfeature_importances_):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420333c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
