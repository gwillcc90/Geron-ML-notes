{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68bbfcde",
   "metadata": {},
   "source": [
    "# Chapter 7: Ensemble Learning and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7bd637",
   "metadata": {},
   "source": [
    "Asking a question to a crowd of thousands of people and aggregating the answer will likely give you a better answer than asking an expert. This is called *wisdom of the crowd*. \n",
    "\n",
    "Similarly, we can aggregate the predictions of a group of predictors (predictors in this case is models: classifiers or regressors) and we get better predictions than with even the best individual predictor. \n",
    "\n",
    "A group of predictors is called an **Ensemble**, thus we have **Ensemble Learning**. An Ensemble Learning algorithm is called an **Ensemble method**.\n",
    "\n",
    "For example, we can train a group of Decision Trees on different subsets of a training set, make predictions, then select the class which gets the most \"votes\" from the group of trees. An ensemble of Decision Trees is called a **Random Forest** and is one of the most powerful ML algorithms available. Ensemble methods frequently win ML competitions.\n",
    "\n",
    "This chapter covers:\n",
    " - Bagging\n",
    " - Boosting\n",
    " - Stacking\n",
    " - etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35708305",
   "metadata": {},
   "source": [
    "## Voting Classifiers\n",
    "Suppose we have some different classification predictors (models), each one achieving around 80% accuracy: LRC, SVM, RFC (random forest), KNN, etc. \n",
    "\n",
    "A simple way to get an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. A majority voting classifier is called a *Hard Voting* classifier.\n",
    "\n",
    "The voting classifier will produce a higher accuracy than the individual classifiers, themselves. \n",
    "\n",
    "Even if our voting classifier is filled with *weak learners* (algorithms that are only slightly better than random guessing), then ensemble is usually still better than a strong learner, provided there are a lot of weak learners and they are diverse. *Ensemble methods work best when the predictors are as independent as possible from one another*.\n",
    "\n",
    "Below see creation of a voting classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b3e6b",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 3 Different (and diverse) classifiers\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "\n",
    "# Ensemble method: VotingClassifier\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), \n",
    "                                          ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "                              voting='hard')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d83a8f",
   "metadata": {},
   "source": [
    "When running this code, the `voting_clf` should at least slightly outperform the individual classifiers.\n",
    "\n",
    "If all classifiers are able to estimate class probabilites (have a predict_proba() method), then SciKit-Learn can predict the class with the highest probability, averaged over the individual classifiers. This is called **soft voting**. Soft voting **achieves higher accuracy than hard voting because it gives higher weight to highly confident votes**.\n",
    "\n",
    "To *use soft voting*, replace `voting = 'hard'` with `voting = 'soft'`\n",
    "\n",
    "Note: SVC does not output probabilities by default; it needs the hyperparameter `probability = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90639752",
   "metadata": {},
   "source": [
    "## Bagging and Pasting\n",
    "With Voting Classifiers, we used diverse (very different) training algorithms (Support vector, Logistic regression, and random forst in the above code).\n",
    "\n",
    "Another approach is to use many of the same training algorithm (predictor), but on different subsets of the training set. \n",
    "\n",
    "**Bagging**, short for *Bootstrap aggregating*, is taking subsets of the training set *with replacement*.\n",
    "\n",
    "**Pasting** is taking subsets of the training set *without replacement*.\n",
    "\n",
    "**Bagging and Pasting allow training instances to be sampled several times over multiple predictors, but ONLY BAGGING allows training instances to be samples several times for the same predictor**.\n",
    "\n",
    "One all predictors are trained, the ensemble makes the prediction by aggregating the individual predictions. The aggregation function is usually the *statistical mode* (the most frequent prediction just like Hard Voting), or the average for a Regression task.\n",
    "\n",
    "The individual predictors have a higher bias than if trained on the full training set, but the ensemble will have a similar bias and a lower variance.\n",
    "\n",
    "Training and predictions in ensemble methods can be done in parallel by allocating CPU cores or using different servers; just another reason ensembles are so strong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b360b92",
   "metadata": {},
   "source": [
    "### Bagging and Pasting in Scikit-Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455e6ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625a814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b0344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
